{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-26 15:09:01.037098: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-26 15:09:01.127847: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-26 15:09:01.127902: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-26 15:09:01.130034: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-26 15:09:01.142754: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-26 15:09:02.464062: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from keras.applications import VGG16  # Import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from keras.preprocessing import image\n",
    "from pyswarms.single.global_best import GlobalBestPSO\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "train_dir = '../malimg_dataset/train'\n",
    "test_dir = '../malimg_dataset/test'\n",
    "image_size = (224, 224, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_images(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label, class_name in enumerate(os.listdir(directory)):\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            for filename in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, filename)\n",
    "                img = image.load_img(img_path, target_size=image_size[:2])  # Resize images to (224, 224)\n",
    "                img_array = image.img_to_array(img)\n",
    "                images.append(img_array)\n",
    "                labels.append(label)\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = load_images(train_dir)\n",
    "test_images, test_labels = load_images(test_dir)\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "cudaSetDevice() on GPU:0 failed. Status: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load VGG16 model without the top classification layer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mVGG16\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimagenet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_top\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Add a global average pooling layer\u001b[39;00m\n\u001b[1;32m      5\u001b[0m x \u001b[38;5;241m=\u001b[39m base_model\u001b[38;5;241m.\u001b[39moutput\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/applications/vgg16.py:154\u001b[0m, in \u001b[0;36mVGG16\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    152\u001b[0m         img_input \u001b[38;5;241m=\u001b[39m input_tensor\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Block 1\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msame\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblock1_conv1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    156\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mConv2D(\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;241m64\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock1_conv2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m )(x)\n\u001b[1;32m    160\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mMaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), strides\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock1_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m)(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/backend.py:2100\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[0;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[1;32m   2098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonce:\n\u001b[1;32m   2099\u001b[0m         seed \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mstateless_fold_in(seed, nonce)\n\u001b[0;32m-> 2100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless_uniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\n\u001b[1;32m   2108\u001b[0m     shape\u001b[38;5;241m=\u001b[39mshape,\n\u001b[1;32m   2109\u001b[0m     minval\u001b[38;5;241m=\u001b[39mminval,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2112\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_legacy_seed(),\n\u001b[1;32m   2113\u001b[0m )\n",
      "\u001b[0;31mInternalError\u001b[0m: cudaSetDevice() on GPU:0 failed. Status: out of memory"
     ]
    }
   ],
   "source": [
    "# Load VGG16 model without the top classification layer\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=image_size)\n",
    "\n",
    "# Add a global average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "model_without_top = Model(inputs=base_model.input, outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 15:23:21.995076: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-04-29 15:23:22.153860: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 11.0.221, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234/234 [==============================] - 16s 46ms/step\n",
      "30/30 [==============================] - 5s 180ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract features using the model without the top layer\n",
    "train_features = model_without_top.predict(train_images)\n",
    "test_features = model_without_top.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 15:43:42,311 - pyswarms.single.global_best - INFO - Optimize for 10 iters with {'c1': 2, 'c2': 2, 'w': 0.9}\n",
      "pyswarms.single.global_best:   0%|          |0/10"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best: 100%|██████████|10/10, best_cost=-0.978\n",
      "2024-04-29 16:05:45,797 - pyswarms.single.global_best - INFO - Optimization finished | best cost: -0.9780564263322884, best pos: [4.60109587 0.25147903 3.1391609 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Parameters:\n",
      "n_estimators = 400, Criterion = gini, Max Depth = 8\n",
      "Accuracy: 0.9728317659352143\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pyswarms as ps\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Discrete values for each parameter\n",
    "n_estimators_options = [50, 100, 200, 300, 400, 500]\n",
    "criterion_options = ['gini', 'entropy']\n",
    "max_depth_options = [2, 4, 6, 8, 10]\n",
    "\n",
    "def objective_function(params):\n",
    "    n_particles = params.shape[0]\n",
    "    accuracies = np.zeros(n_particles)\n",
    "    for i in range(n_particles):\n",
    "        # Map continuous PSO parameters to discrete options\n",
    "        n_estimators_idx = int(params[i, 0])\n",
    "        criterion_idx = int(params[i, 1])\n",
    "        max_depth_idx = int(params[i, 2])\n",
    "\n",
    "        n_estimators = n_estimators_options[n_estimators_idx]\n",
    "        criterion = criterion_options[criterion_idx]\n",
    "        max_depth = max_depth_options[max_depth_idx]\n",
    "\n",
    "        rf_classifier = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth)\n",
    "        rf_classifier.fit(train_features, train_labels)\n",
    "        predictions = rf_classifier.predict(test_features)\n",
    "        accuracies[i] = accuracy_score(test_labels, predictions)\n",
    "\n",
    "    return -accuracies\n",
    "\n",
    "# Define the bounds as indices for the arrays of options\n",
    "bounds = (np.array([0, 0, 0]), np.array([5, 1, 4]))  # Use index bounds\n",
    "\n",
    "# Set up hyperparameters for the PSO algorithm\n",
    "options = {'c1': 2, 'c2': 2, 'w': 0.9}\n",
    "\n",
    "# Create an instance of PSO\n",
    "optimizer = ps.single.GlobalBestPSO(n_particles=10, dimensions=3, options=options, bounds=bounds)\n",
    "\n",
    "# Perform optimization\n",
    "cost, pos = optimizer.optimize(objective_function, iters=10)\n",
    "\n",
    "# Extract the best parameters using indices\n",
    "best_n_estimators = n_estimators_options[int(pos[0])]\n",
    "best_criterion = criterion_options[int(pos[1])]\n",
    "best_max_depth = max_depth_options[int(pos[2])]\n",
    "\n",
    "# Train Random Forest with optimized parameters\n",
    "rf_classifier = RandomForestClassifier(n_estimators=best_n_estimators, criterion=best_criterion, max_depth=best_max_depth)\n",
    "rf_classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(test_labels, rf_classifier.predict(test_features))\n",
    "\n",
    "# Print the results\n",
    "print(\"Optimized Parameters:\")\n",
    "print(f\"n_estimators = {best_n_estimators}, Criterion = {best_criterion}, Max Depth = {best_max_depth}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 16:20:23,655 - pyswarms.single.global_best - INFO - Optimize for 10 iters with {'c1': 2.0, 'c2': 2.0, 'w': 0.9}\n",
      "pyswarms.single.global_best: 100%|██████████|10/10, best_cost=-0.99\n",
      "2024-04-29 16:43:40,637 - pyswarms.single.global_best - INFO - Optimization finished | best cost: -0.9895506792058516, best pos: [0.5194424  2.8692326  0.97393522 0.27683214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Parameters:\n",
      "n_estimators = 100, Max Depth = 30, Min Samples Split = 2, Min Samples Leaf = 1\n",
      "Accuracy: 0.9853709508881923\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pyswarms as ps\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define discrete hyperparameter options\n",
    "n_estimators_options = [100, 300, 500]\n",
    "max_depth_options = [None, 10, 30, 50]\n",
    "min_samples_split_options = [2, 5, 10]\n",
    "min_samples_leaf_options = [1, 2, 4]\n",
    "\n",
    "def objective_function(params):\n",
    "    n_particles = params.shape[0]\n",
    "    accuracies = np.zeros(n_particles)\n",
    "    for i in range(n_particles):\n",
    "        # Convert PSO parameters to indices and select hyperparameters\n",
    "        n_estimators = n_estimators_options[int(params[i, 0])]\n",
    "        max_depth = max_depth_options[int(params[i, 1])]\n",
    "        min_samples_split = min_samples_split_options[int(params[i, 2])]\n",
    "        min_samples_leaf = min_samples_leaf_options[int(params[i, 3])]\n",
    "\n",
    "        rf_classifier = RandomForestClassifier(\n",
    "            n_estimators=n_estimators, \n",
    "            max_depth=max_depth, \n",
    "            min_samples_split=min_samples_split, \n",
    "            min_samples_leaf=min_samples_leaf\n",
    "        )\n",
    "        rf_classifier.fit(train_features, train_labels)\n",
    "        predictions = rf_classifier.predict(test_features)\n",
    "        accuracies[i] = accuracy_score(test_labels, predictions)\n",
    "\n",
    "    return -accuracies\n",
    "\n",
    "# Define the bounds as indices for the arrays of options\n",
    "bounds = (np.array([0, 0, 0, 0]), np.array([2, 3, 2, 2]))  # Use index bounds\n",
    "\n",
    "# Set up hyperparameters for the PSO algorithm\n",
    "options = {'c1': 2.0, 'c2': 2.0, 'w': 0.9}\n",
    "\n",
    "# Create an instance of PSO\n",
    "optimizer = ps.single.GlobalBestPSO(n_particles=10, dimensions=4, options=options, bounds=bounds)\n",
    "\n",
    "# Perform optimization\n",
    "cost, pos = optimizer.optimize(objective_function, iters=10)\n",
    "\n",
    "# Extract the best parameters using indices\n",
    "best_n_estimators = n_estimators_options[int(pos[0])]\n",
    "best_max_depth = max_depth_options[int(pos[1])]\n",
    "best_min_samples_split = min_samples_split_options[int(pos[2])]\n",
    "best_min_samples_leaf = min_samples_leaf_options[int(pos[3])]\n",
    "\n",
    "# Train Random Forest with optimized parameters\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=best_n_estimators, \n",
    "    max_depth=best_max_depth, \n",
    "    min_samples_split=best_min_samples_split, \n",
    "    min_samples_leaf=best_min_samples_leaf\n",
    ")\n",
    "rf_classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(test_labels, rf_classifier.predict(test_features))\n",
    "\n",
    "# Print the results\n",
    "print(\"Optimized Parameters:\")\n",
    "print(f\"n_estimators = {best_n_estimators}, Max Depth = {best_max_depth}, Min Samples Split = {best_min_samples_split}, Min Samples Leaf = {best_min_samples_leaf}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = pipeline.predict(test_features)\n",
    "y_pred = rf_classifier.predict(test_features)\n",
    "\n",
    "# Calculate metrics for SVM classifier\n",
    "precision = precision_score(test_labels, y_pred, average='weighted')\n",
    "recall = recall_score(test_labels, y_pred, average='weighted')\n",
    "f1 = f1_score(test_labels, y_pred, average='weighted')\n",
    "\n",
    "report = classification_report(test_labels, y_pred)\n",
    "\n",
    "# Get confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, y_pred)\n",
    "\n",
    "# Extract TP, TN, FP, FN from confusion matrix\n",
    "# test_labels.shape\n",
    "FP = conf_matrix.sum(axis=0) - np.diag(conf_matrix)  \n",
    "FN = conf_matrix.sum(axis=1) - np.diag(conf_matrix)\n",
    "TP = np.diag(conf_matrix)\n",
    "TN = conf_matrix.sum() - (FP + FN + TP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9852641641621466\n",
      "Recall: 0.9853709508881923\n",
      "F1 Score: 0.984889648034417\n",
      "Precision: 0.9852641641621466\n",
      "Recall: 0.9853709508881923\n",
      "F1 Score: 0.984889648034417\n",
      "TPR (True Positive Rate): 0.9627362637362638\n",
      "TNR (True Negative Rate): 0.9993685092207089\n",
      "FPR (False Positive Rate): 0.0006314907792911876\n",
      "FNR (False Negative Rate): 0.03726373626373626\n",
      "FDR (False Discovery Rate): 0.031074983182365738\n",
      "FOR (False Omission Rate): 0.0005940603907840954\n",
      "MCC (Matthews Correlation Coefficient): 0.9829720263896917\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "tpr = TP / (TP + FN)\n",
    "tnr = TN / (TN + FP)\n",
    "fpr = FP / (TN + FP)\n",
    "fnr = FN / (TP + FN)\n",
    "fdr = FP / (TP + FP)\n",
    "for_ = FN / (TN + FN)\n",
    "# Calculate Matthews Correlation Coefficient (MCC)\n",
    "mcc = matthews_corrcoef(test_labels, y_pred)\n",
    "# mcc = (TP * TN - FP * FN) / np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "# Print metrics\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"TPR (True Positive Rate):\", np.mean(tpr))\n",
    "print(\"TNR (True Negative Rate):\", np.mean(tnr))\n",
    "print(\"FPR (False Positive Rate):\", np.mean(fpr))\n",
    "print(\"FNR (False Negative Rate):\", np.mean(fnr))\n",
    "print(\"FDR (False Discovery Rate):\", np.mean(fdr))\n",
    "print(\"FOR (False Omission Rate):\", np.mean(for_))\n",
    "print(\"MCC (Matthews Correlation Coefficient):\", np.mean(mcc))\n",
    "\n",
    "# print(\"Training Time (SVM):\", train_time_svm, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
